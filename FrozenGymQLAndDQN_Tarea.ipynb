{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Aprendizaje por refuerzos** - FrozenLake y LunarLander (Gymnasium)\n",
        "\n",
        "## Tarea: Implementar Agentes Q-Learning y DQN\n",
        "\n",
        "### Objetivos:\n",
        "1. Implementar el algoritmo Q-Learning\n",
        "2. Implementar el algoritmo DQN\n",
        "3. Entrenar y evaluar ambos agentes\n",
        "4. Comparar el rendimiento de ambos enfoques\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 337,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: swig in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (4.3.1.post0)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (3.10.6)\n",
            "Requirement already satisfied: gymnasium in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (1.2.1)\n",
            "Requirement already satisfied: torch in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.8.0)\n",
            "Requirement already satisfied: pygame in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.6.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (2.2.6)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (3.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gymnasium) (4.14.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: filelock in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (2025.9.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (80.9.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jinja2->torch) (3.0.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Instalar paquetes requeridos\n",
        "import os\n",
        "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
        "\n",
        "%pip install swig matplotlib gymnasium torch pygame\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 338,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importar las bibliotecas\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pygame\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from collections import deque, namedtuple\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La siguiente celda permite ejecutar un juego de Frozen Lake *determinista* para jugar con el teclado.\n",
        "\n",
        "Utilize las teclas de dirección (flechas) o asdw para comandar al agente.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def jugar_frozen_lake(env):\n",
        "    env.reset()\n",
        "    \n",
        "    print(\"Controles:\")\n",
        "    print(\"W - Arriba\")\n",
        "    print(\"S - Abajo\") \n",
        "    print(\"A - Izquierda\")\n",
        "    print(\"D - Derecha\")\n",
        "    print(\"Q - Salir\")\n",
        "    print(\"Presione cualquier tecla para empezar...\")\n",
        "    \n",
        "    pygame.init()\n",
        "    pygame.display.set_caption(\"FrozenLake - Juego Interactivo\")\n",
        "    \n",
        "    clock = pygame.time.Clock()\n",
        "    ejecutando = True\n",
        "    \n",
        "    while ejecutando:\n",
        "        for event in pygame.event.get():\n",
        "            if event.type == pygame.QUIT:\n",
        "                ejecutando = False\n",
        "            elif event.type == pygame.KEYDOWN:\n",
        "                if event.key == pygame.K_q or event.key == pygame.K_ESCAPE:\n",
        "                    ejecutando = False\n",
        "                elif event.key == pygame.K_w or event.key == pygame.K_UP:\n",
        "                    accion = 3  # Arriba\n",
        "                elif event.key == pygame.K_s or event.key == pygame.K_DOWN:\n",
        "                    accion = 1  # Abajo\n",
        "                elif event.key == pygame.K_a or event.key == pygame.K_LEFT:\n",
        "                    accion = 0  # Izquierda\n",
        "                elif event.key == pygame.K_d or event.key == pygame.K_RIGHT:\n",
        "                    accion = 2  # Derecha\n",
        "                else:\n",
        "                    continue\n",
        "                \n",
        "                observacion, recompensa, terminado, truncado, info = env.step(accion)\n",
        "                print(f\"Acción: {accion}, Recompensa: {recompensa}, Terminado: {terminado}\")\n",
        "                \n",
        "                if terminado or truncado:\n",
        "                    print(f\"¡Episodio terminado! Recompensa final: {recompensa}\")\n",
        "                    pygame.time.wait(500)\n",
        "                    env.reset()\n",
        "        \n",
        "        clock.tick(60)\n",
        "    \n",
        "    pygame.quit()\n",
        "    env.close()\n",
        "\n",
        "#env = gym.make('FrozenLake-v1', render_mode='human', is_slippery=False)\n",
        "# Descomente la línea de abajo para jugar interactivamente\n",
        "#jugar_frozen_lake(env)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La siguiente celda permite jugar al juego no determinista."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make('FrozenLake-v1', render_mode='human', is_slippery=True)\n",
        "# Descomente la línea de abajo para jugar interactivamente\n",
        "#jugar_frozen_lake(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La siguiente clase define la interfaz de los agentes que utilizaremos para jugar al Frozen Lake.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 341,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ AgenteAleatorio creado y probado. Acción: 2\n"
          ]
        }
      ],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class Agente(ABC):\n",
        "    \n",
        "    @abstractmethod\n",
        "    def elegir_accion(self, estado):\n",
        "        \"\"\"Elige una acción dada una observación.\"\"\"\n",
        "        pass\n",
        "    \n",
        "    @abstractmethod\n",
        "    def aprender(self, estado, accion, recompensa, siguiente_estado, terminado):\n",
        "        \"\"\"Aprende de la experiencia.\"\"\"\n",
        "        pass\n",
        "\n",
        "class AgenteAleatorio(Agente):\n",
        "    \"\"\"Agente aleatorio que elige acciones al azar.\"\"\"\n",
        "    \n",
        "    def __init__(self, espacio_acciones):\n",
        "        # Se guarda el espacio de acciones para poder elegir acciones al azar\n",
        "        self.espacio_acciones = espacio_acciones\n",
        "    \n",
        "    def elegir_accion(self, estado):\n",
        "        return self.espacio_acciones.sample()\n",
        "    \n",
        "    def aprender(self, estado, accion, recompensa, siguiente_estado, terminado):\n",
        "        pass  # El agente aleatorio no aprende\n",
        "\n",
        "# Probar el AgenteAleatorio\n",
        "env = gym.make('FrozenLake-v1')\n",
        "agente_aleatorio = AgenteAleatorio(env.action_space)\n",
        "estado, _ = env.reset()\n",
        "accion = agente_aleatorio.elegir_accion(estado)\n",
        "print(f\"✓ AgenteAleatorio creado y probado. Acción: {accion}\")\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La siguiente celda define una función para evaluar el desempeño de un agente dado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 342,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Agente Aleatorio - Resultados de Evaluación:\n",
            "Tasa de Victorias: 2.0%\n",
            "Recompensa Promedio: 0.020\n",
            "Desviación Estándar: 0.140\n",
            "Total de Victorias: 2\n"
          ]
        }
      ],
      "source": [
        "# Función de Evaluación de Agentes\n",
        "def evaluar_agente(agente, env, num_episodios=1000):\n",
        "    \"\"\"\n",
        "    Evalúa el rendimiento de un agente a lo largo de múltiples episodios.\n",
        "    \n",
        "    Args:\n",
        "        agente: El agente a evaluar\n",
        "        env: El entorno\n",
        "        num_episodios: Número de episodios a ejecutar\n",
        "    \n",
        "    Returns:\n",
        "        dict: Resultados de la evaluación\n",
        "    \"\"\"\n",
        "    recompensas_totales = []\n",
        "    victorias = 0\n",
        "    \n",
        "    for episodio in range(num_episodios):\n",
        "        estado, _ = env.reset()\n",
        "        recompensa_total = 0\n",
        "        \n",
        "        while True:\n",
        "            accion = agente.elegir_accion(estado)\n",
        "            estado, recompensa, terminado, truncado, _ = env.step(accion)\n",
        "            recompensa_total += recompensa\n",
        "            \n",
        "            if terminado or truncado:\n",
        "                break\n",
        "        \n",
        "        recompensas_totales.append(recompensa_total)\n",
        "        if recompensa_total > 0:\n",
        "            victorias += 1\n",
        "    \n",
        "    return {\n",
        "        'recompensas_totales': recompensas_totales,\n",
        "        'victorias': victorias,\n",
        "        'tasa_victorias': victorias / num_episodios,\n",
        "        'recompensa_promedio': np.mean(recompensas_totales),\n",
        "        'desv_estandar': np.std(recompensas_totales)\n",
        "    }\n",
        "\n",
        "def imprimir_resultados_evaluacion(resultados, nombre_agente):\n",
        "    \"\"\"Imprime los resultados de evaluación de forma formateada.\"\"\"\n",
        "    print(f\"\\n{nombre_agente} - Resultados de Evaluación:\")\n",
        "    print(f\"Tasa de Victorias: {resultados['tasa_victorias']:.1%}\")\n",
        "    print(f\"Recompensa Promedio: {resultados['recompensa_promedio']:.3f}\")\n",
        "    print(f\"Desviación Estándar: {resultados['desv_estandar']:.3f}\")\n",
        "    print(f\"Total de Victorias: {resultados['victorias']}\")\n",
        "\n",
        "# Probar función de evaluación\n",
        "env = gym.make('FrozenLake-v1')\n",
        "agente_aleatorio = AgenteAleatorio(env.action_space)\n",
        "resultados = evaluar_agente(agente_aleatorio, env, num_episodios=100)\n",
        "imprimir_resultados_evaluacion(resultados, \"Agente Aleatorio\")\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La siguiente celda define una función para entrenar un agente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 343,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Funciones de entrenamiento definidas\n"
          ]
        }
      ],
      "source": [
        "# Función de Entrenamiento de Agentes\n",
        "def entrenar_agente(agente, env, num_episodios=1000, max_pasos=100, verbose=True):\n",
        "    \"\"\"\n",
        "    Entrena un agente en el entorno.\n",
        "    \n",
        "    Args:\n",
        "        agente: El agente a entrenar\n",
        "        env: El entorno\n",
        "        num_episodios: Número de episodios de entrenamiento\n",
        "        max_pasos: Máximo de pasos por episodio\n",
        "        verbose: Si imprimir el progreso\n",
        "    \n",
        "    Returns:\n",
        "        list: Recompensas de episodios\n",
        "    \"\"\"\n",
        "    recompensas_episodios = []\n",
        "    longitudes_episodios = []\n",
        "    num_episodios_10 = int(num_episodios / 10)\n",
        "    \n",
        "    for episodio in range(num_episodios):\n",
        "        estado, _ = env.reset()\n",
        "        recompensa_total = 0\n",
        "        pasos = 0\n",
        "        \n",
        "        for paso in range(max_pasos):\n",
        "            accion = agente.elegir_accion(estado)\n",
        "            siguiente_estado, recompensa, terminado, truncado, _ = env.step(accion)\n",
        "            \n",
        "            agente.aprender(estado, accion, recompensa, siguiente_estado, terminado or truncado)\n",
        "            \n",
        "            estado = siguiente_estado\n",
        "            recompensa_total += recompensa\n",
        "            pasos += 1\n",
        "            \n",
        "            if terminado or truncado:\n",
        "                break\n",
        "        \n",
        "        recompensas_episodios.append(recompensa_total)\n",
        "        longitudes_episodios.append(pasos)\n",
        "        \n",
        "        if verbose and (episodio + 1) % num_episodios_10 == 0:\n",
        "            recompensa_promedio = np.mean(recompensas_episodios[-num_episodios_10:])\n",
        "            longitud_promedio = np.mean(longitudes_episodios[-num_episodios_10:])\n",
        "            print(f\"Episodio {episodio + 1}: Recompensa Promedio = {recompensa_promedio:.3f}, Longitud Promedio = {longitud_promedio:.1f}\")\n",
        "    \n",
        "    return recompensas_episodios, longitudes_episodios\n",
        "\n",
        "print(\"✓ Funciones de entrenamiento definidas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La siguiente celda define el agente de Q-Learning a implementar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 344,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Implementar Agente Q-Learning\n",
        "class AgenteQLearning(Agente):\n",
        "    \"\"\"Agente que usa el algoritmo Q-Learning.\"\"\"\n",
        "    \n",
        "    def __init__(self, n_estados, n_acciones, alpha=0.8, gamma=0.95, \n",
        "                 epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
        "        self.n_estados = n_estados\n",
        "        self.n_acciones = n_acciones\n",
        "        self.alpha = alpha          # tasa de aprendizaje\n",
        "        self.gamma = gamma          # descuento futuro\n",
        "        self.epsilon = epsilon      # prob exploración\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        \n",
        "        # Inicializar tabla Q con ceros\n",
        "        self.Q = np.zeros((n_estados, n_acciones))\n",
        "    \n",
        "    def elegir_accion(self, estado):\n",
        "        \"\"\"Elige una acción usando política epsilon-greedy.\"\"\"\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            # Explorar (acción aleatoria)\n",
        "            return np.random.randint(self.n_acciones)\n",
        "        else:\n",
        "            # Explotar (mejor acción conocida)\n",
        "            return np.argmax(self.Q[estado, :])\n",
        "    \n",
        "    def aprender(self, estado, accion, recompensa, siguiente_estado, terminado):\n",
        "        valor_actual = self.Q[estado, accion]\n",
        "        valor_futuro = np.max(self.Q[siguiente_estado, :]) if not (terminado) else 0\n",
        "        self.Q[estado, accion] = valor_actual + self.alpha * (recompensa + self.gamma * valor_futuro - valor_actual)\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aqui deberán incluir código para entrenar y evaluar el agente de Q-Learning implementado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La siguiente celda define el agente DQN a implementar. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 345,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class DQN(nn.Module):\n",
        "    \"\"\"Clase auxiliar que implementa una Red Q Profunda con una capa oculta.\"\"\"\n",
        "    \n",
        "    def __init__(self, tamano_entrada, tamano_oculto, tamano_salida):\n",
        "        super(DQN, self).__init__()\n",
        "        pass\n",
        "    \n",
        "    def forward(self, x):\n",
        "        pass\n",
        "\n",
        "class AgenteDQN(Agente):\n",
        "    \"\"\"Agente de Red Q Profunda.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def elegir_accion(self, estado):\n",
        "        \"\"\"Elige acción usando política epsilon-greedy.\"\"\"\n",
        "        pass\n",
        "    \n",
        "    def aprender(self, estado, accion, recompensa, siguiente_estado, terminado):\n",
        "        pass\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 346,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episodio 1000: Recompensa Promedio = 0.019, Longitud Promedio = 9.3\n",
            "Episodio 2000: Recompensa Promedio = 0.140, Longitud Promedio = 16.7\n",
            "Episodio 3000: Recompensa Promedio = 0.380, Longitud Promedio = 29.6\n",
            "Episodio 4000: Recompensa Promedio = 0.489, Longitud Promedio = 32.4\n",
            "Episodio 5000: Recompensa Promedio = 0.513, Longitud Promedio = 33.9\n",
            "Episodio 6000: Recompensa Promedio = 0.484, Longitud Promedio = 35.4\n",
            "Episodio 7000: Recompensa Promedio = 0.462, Longitud Promedio = 33.4\n",
            "Episodio 8000: Recompensa Promedio = 0.509, Longitud Promedio = 34.5\n",
            "Episodio 9000: Recompensa Promedio = 0.480, Longitud Promedio = 35.9\n",
            "Episodio 10000: Recompensa Promedio = 0.532, Longitud Promedio = 37.4\n",
            "\n",
            "<__main__.AgenteQLearning object at 0x000001695455EFF0> - Resultados de Evaluación:\n",
            "Tasa de Victorias: 70.4%\n",
            "Recompensa Promedio: 0.704\n",
            "Desviación Estándar: 0.456\n",
            "Total de Victorias: 704\n"
          ]
        }
      ],
      "source": [
        "#env = gym.make('FrozenLake-v1', map_name=\"4x4\", is_slippery=False)\n",
        "\n",
        "agente_qlearning = AgenteQLearning(\n",
        "    n_estados=env.observation_space.n,\n",
        "    n_acciones=env.action_space.n,\n",
        "    alpha=0.5,\n",
        "    gamma=0.95,\n",
        "    epsilon=1.0,\n",
        "    epsilon_min=0.01,\n",
        "    epsilon_decay=0.9999\n",
        ")\n",
        "recompensas, longitudes = entrenar_agente(\n",
        "    agente=agente_qlearning,\n",
        "    env=env,\n",
        "    num_episodios=10000,  # más episodios = mejor aprendizaje\n",
        "    max_pasos=100,\n",
        "    verbose=True\n",
        ")\n",
        "res_q = evaluar_agente(agente_qlearning, env)\n",
        "imprimir_resultados_evaluacion(res_q, agente_qlearning)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
