{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Aprendizaje por refuerzos** - FrozenLake y LunarLander (Gymnasium)\n",
        "\n",
        "## Tarea: Implementar Agentes Q-Learning y DQN\n",
        "\n",
        "### Objetivos:\n",
        "1. Implementar el algoritmo Q-Learning\n",
        "2. Implementar el algoritmo DQN\n",
        "3. Entrenar y evaluar ambos agentes\n",
        "4. Comparar el rendimiento de ambos enfoques\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 337,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: swig in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (4.3.1.post0)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (3.10.6)\n",
            "Requirement already satisfied: gymnasium in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (1.2.1)\n",
            "Requirement already satisfied: torch in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.8.0)\n",
            "Requirement already satisfied: pygame in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.6.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (2.2.6)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (3.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gymnasium) (4.14.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: filelock in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (2025.9.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (80.9.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jinja2->torch) (3.0.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Instalar paquetes requeridos\n",
        "import os\n",
        "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
        "\n",
        "%pip install swig matplotlib gymnasium torch pygame\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 338,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importar las bibliotecas\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pygame\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from collections import deque, namedtuple\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La siguiente celda permite ejecutar un juego de Frozen Lake *determinista* para jugar con el teclado.\n",
        "\n",
        "Utilize las teclas de direcci√≥n (flechas) o asdw para comandar al agente.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def jugar_frozen_lake(env):\n",
        "    env.reset()\n",
        "    \n",
        "    print(\"Controles:\")\n",
        "    print(\"W - Arriba\")\n",
        "    print(\"S - Abajo\") \n",
        "    print(\"A - Izquierda\")\n",
        "    print(\"D - Derecha\")\n",
        "    print(\"Q - Salir\")\n",
        "    print(\"Presione cualquier tecla para empezar...\")\n",
        "    \n",
        "    pygame.init()\n",
        "    pygame.display.set_caption(\"FrozenLake - Juego Interactivo\")\n",
        "    \n",
        "    clock = pygame.time.Clock()\n",
        "    ejecutando = True\n",
        "    \n",
        "    while ejecutando:\n",
        "        for event in pygame.event.get():\n",
        "            if event.type == pygame.QUIT:\n",
        "                ejecutando = False\n",
        "            elif event.type == pygame.KEYDOWN:\n",
        "                if event.key == pygame.K_q or event.key == pygame.K_ESCAPE:\n",
        "                    ejecutando = False\n",
        "                elif event.key == pygame.K_w or event.key == pygame.K_UP:\n",
        "                    accion = 3  # Arriba\n",
        "                elif event.key == pygame.K_s or event.key == pygame.K_DOWN:\n",
        "                    accion = 1  # Abajo\n",
        "                elif event.key == pygame.K_a or event.key == pygame.K_LEFT:\n",
        "                    accion = 0  # Izquierda\n",
        "                elif event.key == pygame.K_d or event.key == pygame.K_RIGHT:\n",
        "                    accion = 2  # Derecha\n",
        "                else:\n",
        "                    continue\n",
        "                \n",
        "                observacion, recompensa, terminado, truncado, info = env.step(accion)\n",
        "                print(f\"Acci√≥n: {accion}, Recompensa: {recompensa}, Terminado: {terminado}\")\n",
        "                \n",
        "                if terminado or truncado:\n",
        "                    print(f\"¬°Episodio terminado! Recompensa final: {recompensa}\")\n",
        "                    pygame.time.wait(500)\n",
        "                    env.reset()\n",
        "        \n",
        "        clock.tick(60)\n",
        "    \n",
        "    pygame.quit()\n",
        "    env.close()\n",
        "\n",
        "#env = gym.make('FrozenLake-v1', render_mode='human', is_slippery=False)\n",
        "# Descomente la l√≠nea de abajo para jugar interactivamente\n",
        "#jugar_frozen_lake(env)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La siguiente celda permite jugar al juego no determinista."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make('FrozenLake-v1', render_mode='human', is_slippery=True)\n",
        "# Descomente la l√≠nea de abajo para jugar interactivamente\n",
        "#jugar_frozen_lake(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La siguiente clase define la interfaz de los agentes que utilizaremos para jugar al Frozen Lake.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 341,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì AgenteAleatorio creado y probado. Acci√≥n: 2\n"
          ]
        }
      ],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class Agente(ABC):\n",
        "    \n",
        "    @abstractmethod\n",
        "    def elegir_accion(self, estado):\n",
        "        \"\"\"Elige una acci√≥n dada una observaci√≥n.\"\"\"\n",
        "        pass\n",
        "    \n",
        "    @abstractmethod\n",
        "    def aprender(self, estado, accion, recompensa, siguiente_estado, terminado):\n",
        "        \"\"\"Aprende de la experiencia.\"\"\"\n",
        "        pass\n",
        "\n",
        "class AgenteAleatorio(Agente):\n",
        "    \"\"\"Agente aleatorio que elige acciones al azar.\"\"\"\n",
        "    \n",
        "    def __init__(self, espacio_acciones):\n",
        "        # Se guarda el espacio de acciones para poder elegir acciones al azar\n",
        "        self.espacio_acciones = espacio_acciones\n",
        "    \n",
        "    def elegir_accion(self, estado):\n",
        "        return self.espacio_acciones.sample()\n",
        "    \n",
        "    def aprender(self, estado, accion, recompensa, siguiente_estado, terminado):\n",
        "        pass  # El agente aleatorio no aprende\n",
        "\n",
        "# Probar el AgenteAleatorio\n",
        "env = gym.make('FrozenLake-v1')\n",
        "agente_aleatorio = AgenteAleatorio(env.action_space)\n",
        "estado, _ = env.reset()\n",
        "accion = agente_aleatorio.elegir_accion(estado)\n",
        "print(f\"‚úì AgenteAleatorio creado y probado. Acci√≥n: {accion}\")\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La siguiente celda define una funci√≥n para evaluar el desempe√±o de un agente dado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 342,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Agente Aleatorio - Resultados de Evaluaci√≥n:\n",
            "Tasa de Victorias: 2.0%\n",
            "Recompensa Promedio: 0.020\n",
            "Desviaci√≥n Est√°ndar: 0.140\n",
            "Total de Victorias: 2\n"
          ]
        }
      ],
      "source": [
        "# Funci√≥n de Evaluaci√≥n de Agentes\n",
        "def evaluar_agente(agente, env, num_episodios=1000):\n",
        "    \"\"\"\n",
        "    Eval√∫a el rendimiento de un agente a lo largo de m√∫ltiples episodios.\n",
        "    \n",
        "    Args:\n",
        "        agente: El agente a evaluar\n",
        "        env: El entorno\n",
        "        num_episodios: N√∫mero de episodios a ejecutar\n",
        "    \n",
        "    Returns:\n",
        "        dict: Resultados de la evaluaci√≥n\n",
        "    \"\"\"\n",
        "    recompensas_totales = []\n",
        "    victorias = 0\n",
        "    \n",
        "    for episodio in range(num_episodios):\n",
        "        estado, _ = env.reset()\n",
        "        recompensa_total = 0\n",
        "        \n",
        "        while True:\n",
        "            accion = agente.elegir_accion(estado)\n",
        "            estado, recompensa, terminado, truncado, _ = env.step(accion)\n",
        "            recompensa_total += recompensa\n",
        "            \n",
        "            if terminado or truncado:\n",
        "                break\n",
        "        \n",
        "        recompensas_totales.append(recompensa_total)\n",
        "        if recompensa_total > 0:\n",
        "            victorias += 1\n",
        "    \n",
        "    return {\n",
        "        'recompensas_totales': recompensas_totales,\n",
        "        'victorias': victorias,\n",
        "        'tasa_victorias': victorias / num_episodios,\n",
        "        'recompensa_promedio': np.mean(recompensas_totales),\n",
        "        'desv_estandar': np.std(recompensas_totales)\n",
        "    }\n",
        "\n",
        "def imprimir_resultados_evaluacion(resultados, nombre_agente):\n",
        "    \"\"\"Imprime los resultados de evaluaci√≥n de forma formateada.\"\"\"\n",
        "    print(f\"\\n{nombre_agente} - Resultados de Evaluaci√≥n:\")\n",
        "    print(f\"Tasa de Victorias: {resultados['tasa_victorias']:.1%}\")\n",
        "    print(f\"Recompensa Promedio: {resultados['recompensa_promedio']:.3f}\")\n",
        "    print(f\"Desviaci√≥n Est√°ndar: {resultados['desv_estandar']:.3f}\")\n",
        "    print(f\"Total de Victorias: {resultados['victorias']}\")\n",
        "\n",
        "# Probar funci√≥n de evaluaci√≥n\n",
        "env = gym.make('FrozenLake-v1')\n",
        "agente_aleatorio = AgenteAleatorio(env.action_space)\n",
        "resultados = evaluar_agente(agente_aleatorio, env, num_episodios=100)\n",
        "imprimir_resultados_evaluacion(resultados, \"Agente Aleatorio\")\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La siguiente celda define una funci√≥n para entrenar un agente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 343,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Funciones de entrenamiento definidas\n"
          ]
        }
      ],
      "source": [
        "# Funci√≥n de Entrenamiento de Agentes\n",
        "def entrenar_agente(agente, env, num_episodios=1000, max_pasos=100, verbose=True):\n",
        "    \"\"\"\n",
        "    Entrena un agente en el entorno.\n",
        "    \n",
        "    Args:\n",
        "        agente: El agente a entrenar\n",
        "        env: El entorno\n",
        "        num_episodios: N√∫mero de episodios de entrenamiento\n",
        "        max_pasos: M√°ximo de pasos por episodio\n",
        "        verbose: Si imprimir el progreso\n",
        "    \n",
        "    Returns:\n",
        "        list: Recompensas de episodios\n",
        "    \"\"\"\n",
        "    recompensas_episodios = []\n",
        "    longitudes_episodios = []\n",
        "    num_episodios_10 = int(num_episodios / 10)\n",
        "    \n",
        "    for episodio in range(num_episodios):\n",
        "        estado, _ = env.reset()\n",
        "        recompensa_total = 0\n",
        "        pasos = 0\n",
        "        \n",
        "        for paso in range(max_pasos):\n",
        "            accion = agente.elegir_accion(estado)\n",
        "            siguiente_estado, recompensa, terminado, truncado, _ = env.step(accion)\n",
        "            \n",
        "            agente.aprender(estado, accion, recompensa, siguiente_estado, terminado or truncado)\n",
        "            \n",
        "            estado = siguiente_estado\n",
        "            recompensa_total += recompensa\n",
        "            pasos += 1\n",
        "            \n",
        "            if terminado or truncado:\n",
        "                break\n",
        "        \n",
        "        recompensas_episodios.append(recompensa_total)\n",
        "        longitudes_episodios.append(pasos)\n",
        "        \n",
        "        if verbose and (episodio + 1) % num_episodios_10 == 0:\n",
        "            recompensa_promedio = np.mean(recompensas_episodios[-num_episodios_10:])\n",
        "            longitud_promedio = np.mean(longitudes_episodios[-num_episodios_10:])\n",
        "            print(f\"Episodio {episodio + 1}: Recompensa Promedio = {recompensa_promedio:.3f}, Longitud Promedio = {longitud_promedio:.1f}\")\n",
        "    \n",
        "    return recompensas_episodios, longitudes_episodios\n",
        "\n",
        "print(\"‚úì Funciones de entrenamiento definidas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La siguiente celda define el agente de Q-Learning a implementar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 344,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Implementar Agente Q-Learning\n",
        "class AgenteQLearning(Agente):\n",
        "    \"\"\"Agente que usa el algoritmo Q-Learning.\"\"\"\n",
        "    \n",
        "    def __init__(self, n_estados, n_acciones, alpha=0.8, gamma=0.95, \n",
        "                 epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
        "        self.n_estados = n_estados\n",
        "        self.n_acciones = n_acciones\n",
        "        self.alpha = alpha          # tasa de aprendizaje\n",
        "        self.gamma = gamma          # descuento futuro\n",
        "        self.epsilon = epsilon      # prob exploraci√≥n\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        \n",
        "        # Inicializar tabla Q con ceros\n",
        "        self.Q = np.zeros((n_estados, n_acciones))\n",
        "    \n",
        "    def elegir_accion(self, estado):\n",
        "        \"\"\"Elige una acci√≥n usando pol√≠tica epsilon-greedy.\"\"\"\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            # Explorar (acci√≥n aleatoria)\n",
        "            return np.random.randint(self.n_acciones)\n",
        "        else:\n",
        "            # Explotar (mejor acci√≥n conocida)\n",
        "            return np.argmax(self.Q[estado, :])\n",
        "    \n",
        "    def aprender(self, estado, accion, recompensa, siguiente_estado, terminado):\n",
        "        valor_actual = self.Q[estado, accion]\n",
        "        valor_futuro = np.max(self.Q[siguiente_estado, :]) if not (terminado) else 0\n",
        "        self.Q[estado, accion] = valor_actual + self.alpha * (recompensa + self.gamma * valor_futuro - valor_actual)\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aqui deber√°n incluir c√≥digo para entrenar y evaluar el agente de Q-Learning implementado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La siguiente celda define el agente DQN a implementar. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 345,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class DQN(nn.Module):\n",
        "    \"\"\"Clase auxiliar que implementa una Red Q Profunda con una capa oculta.\"\"\"\n",
        "    \n",
        "    def __init__(self, tamano_entrada, tamano_oculto, tamano_salida):\n",
        "        super(DQN, self).__init__()\n",
        "        pass\n",
        "    \n",
        "    def forward(self, x):\n",
        "        pass\n",
        "\n",
        "class AgenteDQN(Agente):\n",
        "    \"\"\"Agente de Red Q Profunda.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def elegir_accion(self, estado):\n",
        "        \"\"\"Elige acci√≥n usando pol√≠tica epsilon-greedy.\"\"\"\n",
        "        pass\n",
        "    \n",
        "    def aprender(self, estado, accion, recompensa, siguiente_estado, terminado):\n",
        "        pass\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 346,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episodio 1000: Recompensa Promedio = 0.019, Longitud Promedio = 9.3\n",
            "Episodio 2000: Recompensa Promedio = 0.140, Longitud Promedio = 16.7\n",
            "Episodio 3000: Recompensa Promedio = 0.380, Longitud Promedio = 29.6\n",
            "Episodio 4000: Recompensa Promedio = 0.489, Longitud Promedio = 32.4\n",
            "Episodio 5000: Recompensa Promedio = 0.513, Longitud Promedio = 33.9\n",
            "Episodio 6000: Recompensa Promedio = 0.484, Longitud Promedio = 35.4\n",
            "Episodio 7000: Recompensa Promedio = 0.462, Longitud Promedio = 33.4\n",
            "Episodio 8000: Recompensa Promedio = 0.509, Longitud Promedio = 34.5\n",
            "Episodio 9000: Recompensa Promedio = 0.480, Longitud Promedio = 35.9\n",
            "Episodio 10000: Recompensa Promedio = 0.532, Longitud Promedio = 37.4\n",
            "\n",
            "<__main__.AgenteQLearning object at 0x000001695455EFF0> - Resultados de Evaluaci√≥n:\n",
            "Tasa de Victorias: 70.4%\n",
            "Recompensa Promedio: 0.704\n",
            "Desviaci√≥n Est√°ndar: 0.456\n",
            "Total de Victorias: 704\n"
          ]
        }
      ],
      "source": [
        "#env = gym.make('FrozenLake-v1', map_name=\"4x4\", is_slippery=False)\n",
        "\n",
        "agente_qlearning = AgenteQLearning(\n",
        "    n_estados=env.observation_space.n,\n",
        "    n_acciones=env.action_space.n,\n",
        "    alpha=0.5,\n",
        "    gamma=0.95,\n",
        "    epsilon=1.0,\n",
        "    epsilon_min=0.01,\n",
        "    epsilon_decay=0.9999\n",
        ")\n",
        "recompensas, longitudes = entrenar_agente(\n",
        "    agente=agente_qlearning,\n",
        "    env=env,\n",
        "    num_episodios=10000,  # m√°s episodios = mejor aprendizaje\n",
        "    max_pasos=100,\n",
        "    verbose=True\n",
        ")\n",
        "res_q = evaluar_agente(agente_qlearning, env)\n",
        "imprimir_resultados_evaluacion(res_q, agente_qlearning)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
